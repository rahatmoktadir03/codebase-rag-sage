{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![Img](https://app.theheadstarter.com/static/hs-logo-opengraph.png)\n",
        "\n",
        "# Headstarter Codebase RAG Project"
      ],
      "metadata": {
        "id": "FTmVgAC90r3Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Screenshot 2024-11-25 at 7 12 58 PM](https://github.com/user-attachments/assets/0bd67cf0-43d5-46d2-879c-a752cae4c8e3)"
      ],
      "metadata": {
        "id": "JSQbb-WI0Nb2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Necessary Libraries"
      ],
      "metadata": {
        "id": "MpmkP4rM1KRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pygithub langchain langchain-community openai groq tiktoken pinecone-client langchain_pinecone sentence-transformers"
      ],
      "metadata": {
        "id": "BGFWnzpBDkWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAIXpUxWDFSV"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from google.colab import userdata\n",
        "from pinecone import Pinecone\n",
        "import os\n",
        "import tempfile\n",
        "from github import Github, Repository\n",
        "from git import Repo\n",
        "from openai import OpenAI\n",
        "from groq import Groq\n",
        "from pathlib import Path\n",
        "from langchain.schema import Document\n",
        "from pinecone import Pinecone"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clone a GitHub Repo locally"
      ],
      "metadata": {
        "id": "hTLsQ9Ma1FpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clone_repository(repo_url):\n",
        "    \"\"\"Clones a GitHub repository to a temporary directory.\n",
        "\n",
        "    Args:\n",
        "        repo_url: The URL of the GitHub repository.\n",
        "\n",
        "    Returns:\n",
        "        The path to the cloned repository.\n",
        "    \"\"\"\n",
        "    repo_name = repo_url.split(\"/\")[-1]  # Extract repository name from URL\n",
        "    repo_path = f\"/content/{repo_name}\"\n",
        "    Repo.clone_from(repo_url, str(repo_path))\n",
        "    return str(repo_path)"
      ],
      "metadata": {
        "id": "kKioMYZBDee4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = clone_repository(\"https://github.com/CoderAgent/SecureAgent\")"
      ],
      "metadata": {
        "id": "F_1zslPsDmJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(path)"
      ],
      "metadata": {
        "id": "hFrrr5rjEfYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SUPPORTED_EXTENSIONS = {'.py', '.js', '.tsx', '.jsx', '.ipynb', '.java',\n",
        "                         '.cpp', '.ts', '.go', '.rs', '.vue', '.swift', '.c', '.h'}\n",
        "\n",
        "IGNORED_DIRS = {'node_modules', 'venv', 'env', 'dist', 'build', '.git',\n",
        "                '__pycache__', '.next', '.vscode', 'vendor'}"
      ],
      "metadata": {
        "id": "MQOcyi6DE5bL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_file_content(file_path, repo_path):\n",
        "    \"\"\"\n",
        "    Get content of a single file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the file\n",
        "\n",
        "    Returns:\n",
        "        Optional[Dict[str, str]]: Dictionary with file name and content\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "\n",
        "        # Get relative path from repo root\n",
        "        rel_path = os.path.relpath(file_path, repo_path)\n",
        "\n",
        "        return {\n",
        "            \"name\": rel_path,\n",
        "            \"content\": content\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {file_path}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def get_main_files_content(repo_path: str):\n",
        "    \"\"\"\n",
        "    Get content of supported code files from the local repository.\n",
        "\n",
        "    Args:\n",
        "        repo_path: Path to the local repository\n",
        "\n",
        "    Returns:\n",
        "        List of dictionaries containing file names and contents\n",
        "    \"\"\"\n",
        "    files_content = []\n",
        "\n",
        "    try:\n",
        "        for root, _, files in os.walk(repo_path):\n",
        "            # Skip if current directory is in ignored directories\n",
        "            if any(ignored_dir in root for ignored_dir in IGNORED_DIRS):\n",
        "                continue\n",
        "\n",
        "            # Process each file in current directory\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                if os.path.splitext(file)[1] in SUPPORTED_EXTENSIONS:\n",
        "                    file_content = get_file_content(file_path, repo_path)\n",
        "                    if file_content:\n",
        "                        files_content.append(file_content)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading repository: {str(e)}\")\n",
        "\n",
        "    return files_content"
      ],
      "metadata": {
        "id": "qi0FbfdrF6Hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_content = get_main_files_content(path)"
      ],
      "metadata": {
        "id": "9-x-fTUqHISX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_content"
      ],
      "metadata": {
        "id": "drDUCMcWIC-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings"
      ],
      "metadata": {
        "id": "fTHEOUgp1Nmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_huggingface_embeddings(text, model_name=\"sentence-transformers/all-mpnet-base-v2\"):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    return model.encode(text)"
      ],
      "metadata": {
        "id": "pRz7UnvJoL-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I am a programmer\"\n",
        "\n",
        "embeddings = get_huggingface_embeddings(text)"
      ],
      "metadata": {
        "id": "ojCvJduqIEQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings"
      ],
      "metadata": {
        "id": "6Oas_olkZSkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oe-7UwHGvCno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up Pinecone\n",
        "**1. Create an account on [Pinecone.io](https://app.pinecone.io/)**\n",
        "\n",
        "**2. Create a new index called \"codebase-rag\" and set the dimensions to 768. Leave the rest of the settings as they are.**\n",
        "\n",
        "![Screenshot 2024-11-24 at 10 58 50 PM](https://github.com/user-attachments/assets/f5fda046-4087-432a-a8c2-86e061005238)\n",
        "\n",
        "\n",
        "\n",
        "**3. Create an API Key for Pinecone**\n",
        "\n",
        "![Screenshot 2024-11-24 at 10 44 37 PM](https://github.com/user-attachments/assets/e7feacc6-2bd1-472a-82e5-659f65624a88)\n",
        "\n",
        "\n",
        "**4. Store your Pinecone API Key within Google Colab's secrets section, and then enable access to it (see the blue checkmark)**\n",
        "\n",
        "![Screenshot 2024-11-24 at 10 45 25 PM](https://github.com/user-attachments/assets/eaf73083-0b5f-4d17-9e0c-eab84f91b0bc)\n",
        "\n"
      ],
      "metadata": {
        "id": "umKbNfk3aBOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the PINECONE_API_KEY as an environment variable\n",
        "pinecone_api_key = userdata.get(\"PINECONE_API_KEY\")\n",
        "os.environ['PINECONE_API_KEY'] = pinecone_api_key\n",
        "\n",
        "# Initialize Pinecone\n",
        "pc = Pinecone(api_key=userdata.get(\"PINECONE_API_KEY\"),)\n",
        "\n",
        "# Connect to your Pinecone index\n",
        "pinecone_index = pc.Index(\"codebase-rag\")"
      ],
      "metadata": {
        "id": "y05YK2IjaGgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = PineconeVectorStore(index_name=\"codebase-rag\", embedding=HuggingFaceEmbeddings())"
      ],
      "metadata": {
        "id": "OQN1SdEQbwDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = []\n",
        "\n",
        "for file in file_content:\n",
        "    doc = Document(\n",
        "        page_content=f\"{file['name']}\\n{file['content']}\",\n",
        "        metadata={\"source\": file['name']}\n",
        "    )\n",
        "\n",
        "    documents.append(doc)\n",
        "\n",
        "\n",
        "vectorstore = PineconeVectorStore.from_documents(\n",
        "    documents=documents,\n",
        "    embedding=HuggingFaceEmbeddings(),\n",
        "    index_name=\"codebase-rag\",\n",
        "    namespace=\"https://github.com/CoderAgent/SecureAgent\"\n",
        ")"
      ],
      "metadata": {
        "id": "tDAB_siIb93B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TGuQiFQmd4HZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform RAG\n",
        "\n",
        "1. Get your Groq API Key [here](https://console.groq.com/keys)\n",
        "\n",
        "2. Paste your Groq API Key into your Google Colab secrets, and make sure to enable permissions for it\n",
        "\n",
        "![Screenshot 2024-11-25 at 12 00 16 AM](https://github.com/user-attachments/assets/e5525d29-bca6-4dbd-892b-cc770a6b281d)\n"
      ],
      "metadata": {
        "id": "e75xrBVCrRL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = Groq(\n",
        "    base_url=\"https://api.groq.com\",\n",
        "    api_key=userdata.get(\"GROQ_API_KEY\")\n",
        ")"
      ],
      "metadata": {
        "id": "K9DJQMc_nrsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How are python files parsed?\""
      ],
      "metadata": {
        "id": "73ahsGAUnY22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_query_embedding = get_huggingface_embeddings(query)\n",
        "\n",
        "raw_query_embedding"
      ],
      "metadata": {
        "id": "Lwg3ZpsHnhhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feel free to change the \"top_k\" parameter to be a higher or lower number\n",
        "top_matches = pinecone_index.query(vector=raw_query_embedding.tolist(), top_k=5, include_metadata=True, namespace=\"https://github.com/CoderAgent/SecureAgent\")"
      ],
      "metadata": {
        "id": "5DQKsXMunhkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_matches"
      ],
      "metadata": {
        "id": "_6JOvq6Tnhmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contexts = [item['metadata']['text'] for item in top_matches['matches']]"
      ],
      "metadata": {
        "id": "ahEZbqcAnhox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contexts"
      ],
      "metadata": {
        "id": "VOwLKCRNqKF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_query = \"<CONTEXT>\\n\" + \"\\n\\n-------\\n\\n\".join(contexts[ : 10]) + \"\\n-------\\n</CONTEXT>\\n\\n\\n\\nMY QUESTION:\\n\" + query"
      ],
      "metadata": {
        "id": "gHWR6szqqK7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(augmented_query)"
      ],
      "metadata": {
        "id": "AROG6MwJqNnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = f\"\"\"You are a Senior Software Engineer, specializing in TypeScript.\n",
        "\n",
        "Answer any questions I have about the codebase, based on the code provided. Always consider all of the context provided when forming a response.\n",
        "\"\"\"\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": augmented_query}\n",
        "    ],\n",
        "    model=\"llama3-8b-8192\",\n",
        ")\n",
        "\n",
        "response = chat_completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "Q-l0gwYHqNpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "id": "reEO69LpqNr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MqJtdpK_qNut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Putting it all together"
      ],
      "metadata": {
        "id": "OBi56NBnjYMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_rag(query):\n",
        "    raw_query_embedding = get_huggingface_embeddings(query)\n",
        "\n",
        "    top_matches = pinecone_index.query(vector=raw_query_embedding.tolist(), top_k=5, include_metadata=True, namespace=\"https://github.com/CoderAgent/SecureAgent\")\n",
        "\n",
        "    # Get the list of retrieved texts\n",
        "    contexts = [item['metadata']['text'] for item in top_matches['matches']]\n",
        "\n",
        "    augmented_query = \"<CONTEXT>\\n\" + \"\\n\\n-------\\n\\n\".join(contexts[ : 10]) + \"\\n-------\\n</CONTEXT>\\n\\n\\n\\nMY QUESTION:\\n\" + query\n",
        "\n",
        "    # Modify the prompt below as need to improve the response quality\n",
        "    system_prompt = f\"\"\"You are a Senior Software Engineer, specializing in TypeScript.\n",
        "\n",
        "    Answer any questions I have about the codebase, based on the code provided. Always consider all of the context provided when forming a response.\n",
        "    \"\"\"\n",
        "\n",
        "    llm_response = client.chat.completions.create(\n",
        "        model=\"llama-3.1-8b-instant\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": augmented_query}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return llm_response.choices[0].message.content"
      ],
      "metadata": {
        "id": "SUeLm0_uqkDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = perform_rag(\"How is the javascript parser used?\")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "VWTmDGI-qkFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7FwqRviPqkHy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
